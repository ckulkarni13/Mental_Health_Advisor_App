import os
from pinecone import Pinecone, ServerlessSpec, Index
from dotenv import load_dotenv
from openai import OpenAI

# Load environment variables
load_dotenv()

# Initialize Pinecone
pinecone_api_key = os.getenv("PINECONE_API_KEY")
index_name = "mental-health-index"

pc = Pinecone(api_key=pinecone_api_key)

# Check if the index exists
if index_name not in pc.list_indexes().names():
    raise ValueError(f"Index '{index_name}' does not exist. Please ensure it is created and populated.")

# Connect to Pinecone index
index = pc.Index(index_name)

# OpenAI Initialization
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def generate_query_embedding(query):
    """
    Generate embeddings for the user's query using OpenAI's text-embedding-ada-002 model.
    """
    try:
        response = client.embeddings.create(model="text-embedding-ada-002", input=query)
        return response.data[0].embedding
    except Exception as e:
        print(f"Failed to generate query embedding: {e}")
        return None

def query_pinecone(query_embedding, pinecone_index, top_k=5, similarity_threshold=0.8):
    """
    Query Pinecone to retrieve the most relevant vectors based on the query embedding.
    Include a similarity threshold to filter out irrelevant matches.

    Args:
        query_embedding (list): The query embedding.
        pinecone_index (Index): The Pinecone index.
        top_k (int): Number of top results to retrieve.
        similarity_threshold (float): Minimum similarity score to consider a match relevant.

    Returns:
        list: A list of the top matches' metadata from Pinecone or an empty list if no match is relevant.
    """
    try:
        results = pinecone_index.query(
            vector=query_embedding,
            top_k=top_k,
            include_metadata=True,
            include_values=False
        )
        # Filter results based on similarity threshold
        relevant_results = [
            match["metadata"] for match in results["matches"]
            if match["score"] >= similarity_threshold
        ]

        if relevant_results:
            return relevant_results
        else:
            print("No relevant matches found.")
            return []
    except Exception as e:
        print(f"Error querying Pinecone: {e}")
        return []

def generate_advice_with_gpt4(query, context):
    """
    Generate a detailed response using GPT-4 based on the query and retrieved context.
    """
    try:
        context_combined = "\n".join([c.get('response', '') for c in context])
        messages = [
            {"role": "system", "content": "You are a professional assistant providing detailed and empathetic advice."},
            {"role": "user", "content": f"""
            You are an empathetic and professional assistant providing support to mental health counselors.
            Based on the query and the retrieved context, provide a detailed, empathetic, and actionable response.

            User Query: {query}

            Retrieved Context:
            {context_combined}

            Response:
            """}
        ]
        response = client.chat.completions.create(
            model="gpt-4",
            messages=messages
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error generating response with GPT-4: {e}")
        return "Sorry, I couldn't generate a response at the moment."

def handle_user_query(user_query, pinecone_index):
    """
    Handle the user's query by retrieving relevant context from Pinecone and generating advice with GPT-4.

    Args:
        user_query (str): The query provided by the user.
        pinecone_index (Index): The Pinecone index.

    Returns:
        str: The final advice generated by GPT-4 or a message indicating no relevant data found.
    """
    # Generate query embedding
    query_embedding = generate_query_embedding(user_query)
    if not query_embedding:
        return "Failed to generate embedding for the query."

    # Retrieve relevant context from Pinecone
    retrieved_context = query_pinecone(query_embedding, pinecone_index, top_k=5)
    if not retrieved_context:
        return "No relevant data found for the query. Please ask a question related to mental health counseling."

    # Generate advice with GPT-4
    final_advice = generate_advice_with_gpt4(user_query, retrieved_context)
    return final_advice

# Example Usage
if __name__ == "__main__":
    user_query = input("Enter your query: ")
    advice = handle_user_query(user_query, index)
    print("\nGenerated Advice:\n", advice)
